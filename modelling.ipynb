{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa5507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c661407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 NaN values in Close\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farhan Ramadhan\\AppData\\Local\\Temp\\ipykernel_24696\\1125058606.py:18: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data[target_col] = data[target_col].fillna(method='ffill')  # Forward fill\n",
      "C:\\Users\\Farhan Ramadhan\\AppData\\Local\\Temp\\ipykernel_24696\\1125058606.py:19: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data[target_col] = data[target_col].fillna(method='bfill')  # Backward fill for any remaining NaNs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "78/78 [==============================] - 13s 87ms/step - loss: 0.0013 - val_loss: 0.0010\n",
      "Epoch 2/50\n",
      "78/78 [==============================] - 4s 53ms/step - loss: 3.9040e-04 - val_loss: 8.7251e-04\n",
      "Epoch 3/50\n",
      "78/78 [==============================] - 5s 64ms/step - loss: 3.2103e-04 - val_loss: 0.0014\n",
      "Epoch 4/50\n",
      "78/78 [==============================] - 5s 63ms/step - loss: 3.2363e-04 - val_loss: 7.2609e-04\n",
      "Epoch 5/50\n",
      "78/78 [==============================] - 4s 55ms/step - loss: 2.3696e-04 - val_loss: 6.6732e-04\n",
      "Epoch 6/50\n",
      "78/78 [==============================] - 5s 58ms/step - loss: 2.3183e-04 - val_loss: 8.3727e-04\n",
      "Epoch 7/50\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 2.5176e-04 - val_loss: 5.8918e-04\n",
      "Epoch 8/50\n",
      "78/78 [==============================] - 4s 53ms/step - loss: 2.1553e-04 - val_loss: 7.6198e-04\n",
      "Epoch 9/50\n",
      "78/78 [==============================] - 4s 53ms/step - loss: 2.5924e-04 - val_loss: 5.4068e-04\n",
      "Epoch 10/50\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 2.1391e-04 - val_loss: 5.5201e-04\n",
      "Epoch 11/50\n",
      "78/78 [==============================] - 4s 55ms/step - loss: 2.3073e-04 - val_loss: 8.2769e-04\n",
      "Epoch 12/50\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 1.9657e-04 - val_loss: 9.3824e-04\n",
      "Epoch 13/50\n",
      "78/78 [==============================] - 4s 55ms/step - loss: 2.3480e-04 - val_loss: 5.7417e-04\n",
      "Epoch 14/50\n",
      "78/78 [==============================] - 4s 56ms/step - loss: 2.0833e-04 - val_loss: 4.4987e-04\n",
      "Epoch 15/50\n",
      "78/78 [==============================] - 5s 59ms/step - loss: 1.7977e-04 - val_loss: 4.6341e-04\n",
      "Epoch 16/50\n",
      "78/78 [==============================] - 4s 56ms/step - loss: 2.1389e-04 - val_loss: 4.3816e-04\n",
      "Epoch 17/50\n",
      "78/78 [==============================] - 5s 67ms/step - loss: 2.3220e-04 - val_loss: 7.8151e-04\n",
      "Epoch 18/50\n",
      "78/78 [==============================] - 5s 59ms/step - loss: 2.1594e-04 - val_loss: 0.0016\n",
      "Epoch 19/50\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 1.9239e-04 - val_loss: 3.9940e-04\n",
      "Epoch 20/50\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 1.8716e-04 - val_loss: 3.7204e-04\n",
      "Epoch 21/50\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 1.6480e-04 - val_loss: 4.0431e-04\n",
      "Epoch 22/50\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 1.6184e-04 - val_loss: 3.3660e-04\n",
      "Epoch 23/50\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 1.8675e-04 - val_loss: 4.1860e-04\n",
      "Epoch 24/50\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 1.6537e-04 - val_loss: 3.6803e-04\n",
      "Epoch 25/50\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 1.8873e-04 - val_loss: 0.0012\n",
      "Epoch 26/50\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 1.6180e-04 - val_loss: 3.0938e-04\n",
      "Epoch 27/50\n",
      "78/78 [==============================] - 4s 51ms/step - loss: 1.8901e-04 - val_loss: 4.6671e-04\n",
      "Epoch 28/50\n",
      "78/78 [==============================] - 4s 49ms/step - loss: 1.8973e-04 - val_loss: 3.1030e-04\n",
      "Epoch 29/50\n",
      "78/78 [==============================] - 4s 56ms/step - loss: 1.8116e-04 - val_loss: 7.3974e-04\n",
      "Epoch 30/50\n",
      "78/78 [==============================] - 5s 64ms/step - loss: 1.6132e-04 - val_loss: 4.8178e-04\n",
      "Epoch 31/50\n",
      "78/78 [==============================] - 5s 66ms/step - loss: 1.6029e-04 - val_loss: 3.0703e-04\n",
      "Epoch 32/50\n",
      "78/78 [==============================] - 5s 65ms/step - loss: 1.8777e-04 - val_loss: 2.9010e-04\n",
      "Epoch 33/50\n",
      "78/78 [==============================] - 5s 66ms/step - loss: 1.5700e-04 - val_loss: 2.9253e-04\n",
      "Epoch 34/50\n",
      "78/78 [==============================] - 5s 67ms/step - loss: 1.7460e-04 - val_loss: 3.1250e-04\n",
      "Epoch 35/50\n",
      "78/78 [==============================] - 4s 57ms/step - loss: 1.8660e-04 - val_loss: 2.9085e-04\n",
      "Epoch 36/50\n",
      "78/78 [==============================] - 5s 60ms/step - loss: 1.7145e-04 - val_loss: 3.2564e-04\n",
      "Epoch 37/50\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 1.6889e-04 - val_loss: 4.5546e-04\n",
      "Epoch 38/50\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 1.4086e-04 - val_loss: 2.7787e-04\n",
      "Epoch 39/50\n",
      "78/78 [==============================] - 5s 58ms/step - loss: 1.4874e-04 - val_loss: 5.6747e-04\n",
      "Epoch 40/50\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 1.7849e-04 - val_loss: 3.0717e-04\n",
      "Epoch 41/50\n",
      "78/78 [==============================] - 5s 59ms/step - loss: 1.8993e-04 - val_loss: 3.2861e-04\n",
      "Epoch 42/50\n",
      "78/78 [==============================] - 4s 56ms/step - loss: 1.6850e-04 - val_loss: 3.1753e-04\n",
      "Epoch 43/50\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 1.7056e-04 - val_loss: 2.5656e-04\n",
      "Epoch 44/50\n",
      "78/78 [==============================] - 4s 54ms/step - loss: 1.4394e-04 - val_loss: 6.5955e-04\n",
      "Epoch 45/50\n",
      "78/78 [==============================] - 4s 52ms/step - loss: 1.5483e-04 - val_loss: 2.6115e-04\n",
      "Epoch 46/50\n",
      "78/78 [==============================] - 4s 55ms/step - loss: 2.0481e-04 - val_loss: 2.5664e-04\n",
      "Epoch 47/50\n",
      "78/78 [==============================] - 5s 61ms/step - loss: 1.8184e-04 - val_loss: 2.6950e-04\n",
      "Epoch 48/50\n",
      "78/78 [==============================] - 4s 57ms/step - loss: 1.5618e-04 - val_loss: 7.7292e-04\n",
      "Epoch 49/50\n",
      "78/78 [==============================] - 4s 56ms/step - loss: 2.0230e-04 - val_loss: 2.5341e-04\n",
      "Epoch 50/50\n",
      "78/78 [==============================] - 7s 94ms/step - loss: 1.7404e-04 - val_loss: 3.3488e-04\n",
      "38/38 [==============================] - 2s 18ms/step\n",
      "\n",
      "Model Metrics:\n",
      "{'LSTM': {'MSE': 19347714.063588604, 'MAE': 2970.4302283022585, 'R2': 0.9815194308490012}, 'RF': {'MSE': 388512327.75292975, 'MAE': 10819.388262687111, 'R2': 0.6289004005612326}}\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "\n",
      "Predictions for next day:\n",
      "LSTM: $106406.53\n",
      "RF: $64540.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farhan Ramadhan\\AppData\\Local\\Temp\\ipykernel_24696\\1125058606.py:121: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data[target_col] = data[target_col].fillna(method='ffill')\n",
      "C:\\Users\\Farhan Ramadhan\\AppData\\Local\\Temp\\ipykernel_24696\\1125058606.py:122: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data[target_col] = data[target_col].fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "class FinancialPredictor:\n",
    "    def __init__(self):\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.lstm_model = None\n",
    "        self.rf_model = None\n",
    "        self.sequence_length = 60  # Number of time steps to look back\n",
    "        \n",
    "    def prepare_data(self, data, target_col='Close'):\n",
    "        \"\"\"Prepare data for modeling\"\"\"\n",
    "        # Make a copy to avoid modifying original data\n",
    "        data = data.copy()\n",
    "        \n",
    "        # Check for NaN values before processing\n",
    "        if data[target_col].isnull().any():\n",
    "            print(f\"Found {data[target_col].isnull().sum()} NaN values in {target_col}\")\n",
    "        \n",
    "        # Handle missing values\n",
    "        data[target_col] = data[target_col].fillna(method='ffill')  # Forward fill\n",
    "        data[target_col] = data[target_col].fillna(method='bfill')  # Backward fill for any remaining NaNs\n",
    "        \n",
    "        # Verify no NaN values remain\n",
    "        if data[target_col].isnull().any():\n",
    "            raise ValueError(\"Unable to handle all NaN values in the data\")\n",
    "        \n",
    "        # Scale the data\n",
    "        scaled_data = self.scaler.fit_transform(data[[target_col]])\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(self.sequence_length, len(scaled_data)):\n",
    "            X.append(scaled_data[i-self.sequence_length:i])\n",
    "            y.append(scaled_data[i])\n",
    "        \n",
    "        X, y = np.array(X), np.array(y)\n",
    "        \n",
    "        # Split for LSTM (3D shape)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "        \n",
    "        # Reshape for Random Forest (2D shape)\n",
    "        X_rf = X.reshape(X.shape[0], -1)\n",
    "        X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, y, test_size=0.3, shuffle=False)\n",
    "        \n",
    "        return (X_train, X_test, y_train, y_test), (X_train_rf, X_test_rf, y_train_rf, y_test_rf)\n",
    "    \n",
    "    def build_lstm_model(self, input_shape):\n",
    "        \"\"\"Build LSTM model\"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(50, return_sequences=True, input_shape=input_shape),\n",
    "            Dropout(0.2),\n",
    "            LSTM(50, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(25),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        return model\n",
    "\n",
    "    def train_models(self, data, target_col='Close'):\n",
    "        \"\"\"Train both LSTM and Random Forest models\"\"\"\n",
    "        # Prepare data\n",
    "        lstm_data, rf_data = self.prepare_data(data, target_col)\n",
    "        (X_train, X_test, y_train, y_test) = lstm_data\n",
    "        (X_train_rf, X_test_rf, y_train_rf, y_test_rf) = rf_data\n",
    "        \n",
    "        # Train LSTM\n",
    "        self.lstm_model = self.build_lstm_model((X_train.shape[1], 1))\n",
    "        self.lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)\n",
    "        \n",
    "        # Train Random Forest\n",
    "        self.rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        self.rf_model.fit(X_train_rf, y_train_rf.ravel())\n",
    "        \n",
    "        # Evaluate models\n",
    "        lstm_predictions = self.lstm_model.predict(X_test)\n",
    "        rf_predictions = self.rf_model.predict(X_test_rf)\n",
    "        \n",
    "        # Convert predictions back to original scale\n",
    "        lstm_predictions = self.scaler.inverse_transform(lstm_predictions)\n",
    "        rf_predictions = self.scaler.inverse_transform(rf_predictions.reshape(-1, 1))\n",
    "        y_test_orig = self.scaler.inverse_transform(y_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'LSTM': {\n",
    "                'MSE': mean_squared_error(y_test_orig, lstm_predictions),\n",
    "                'MAE': mean_absolute_error(y_test_orig, lstm_predictions),\n",
    "                'R2': r2_score(y_test_orig, lstm_predictions)\n",
    "            },\n",
    "            'RF': {\n",
    "                'MSE': mean_squared_error(y_test_orig, rf_predictions),\n",
    "                'MAE': mean_absolute_error(y_test_orig, rf_predictions),\n",
    "                'R2': r2_score(y_test_orig, rf_predictions)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def save_models(self, symbol):\n",
    "        \"\"\"Save trained models\"\"\"\n",
    "        if not os.path.exists('models'):\n",
    "            os.makedirs('models')\n",
    "            \n",
    "        # Save LSTM model\n",
    "        self.lstm_model.save(f'models/lstm_{symbol}.h5')\n",
    "        \n",
    "        # Save Random Forest model\n",
    "        joblib.dump(self.rf_model, f'models/rf_{symbol}.joblib')\n",
    "        \n",
    "        # Save scaler\n",
    "        joblib.dump(self.scaler, f'models/scaler_{symbol}.joblib')\n",
    "\n",
    "    def load_models(self, symbol):\n",
    "        \"\"\"Load trained models\"\"\"\n",
    "        self.lstm_model = load_model(f'models/lstm_{symbol}.h5')\n",
    "        self.rf_model = joblib.load(f'models/rf_{symbol}.joblib')\n",
    "        self.scaler = joblib.load(f'models/scaler_{symbol}.joblib')\n",
    "\n",
    "    def predict(self, data, target_col='Close'):\n",
    "        \"\"\"Make predictions using both models\"\"\"\n",
    "        data = data.copy()\n",
    "        data[target_col] = data[target_col].fillna(method='ffill')\n",
    "        data[target_col] = data[target_col].fillna(method='bfill')\n",
    "     \n",
    "        # Prepare data\n",
    "        scaled_data = self.scaler.transform(data[[target_col]])\n",
    "        \n",
    "        # Prepare sequence for LSTM\n",
    "        sequence = scaled_data[-self.sequence_length:]\n",
    "        sequence = sequence.reshape(1, self.sequence_length, 1)\n",
    "        \n",
    "        # Prepare data for Random Forest\n",
    "        rf_input = sequence.reshape(1, -1)\n",
    "        \n",
    "        # Make predictions\n",
    "        lstm_pred = self.lstm_model.predict(sequence)\n",
    "        rf_pred = self.rf_model.predict(rf_input)\n",
    "        \n",
    "        # Convert predictions back to original scale\n",
    "        lstm_pred = self.scaler.inverse_transform(lstm_pred)\n",
    "        rf_pred = self.scaler.inverse_transform(rf_pred.reshape(-1, 1))\n",
    "        \n",
    "        return {\n",
    "            'LSTM': lstm_pred[0][0],\n",
    "            'RF': rf_pred[0][0]\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load processed data\n",
    "    symbol = \"BTC\"  # Example symbol\n",
    "    data = pd.read_csv(f'data/processed/crypto/{symbol}_processed.csv')\n",
    "    \n",
    "    # Initialize predictor\n",
    "    predictor = FinancialPredictor()\n",
    "    \n",
    "    # Train models\n",
    "    metrics = predictor.train_models(data)\n",
    "    print(\"\\nModel Metrics:\")\n",
    "    print(metrics)\n",
    "    \n",
    "    # Save models\n",
    "    predictor.save_models(symbol)\n",
    "    \n",
    "    # Example of making a prediction\n",
    "    last_60_days = data.tail(60)\n",
    "    predictions = predictor.predict(last_60_days)\n",
    "    print(f\"\\nPredictions for next day:\")\n",
    "    print(f\"LSTM: ${predictions['LSTM']:.2f}\")\n",
    "    print(f\"RF: ${predictions['RF']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
